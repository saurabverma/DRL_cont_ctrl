{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continous Control Project - Report\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, a solution to the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) is provided. Specifically, a Deep Deterministic Policy Gradients is trained to play the Unity-Reacher game and maximize the reward score.\n",
    "\n",
    "This jupyter notebook is designed to serve as report for the project, by providing all necessary details of the learning algorithm used.\n",
    "At the same time, the notebook also serves as training/testing of the agent to play the game.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "- A Deep Deterministic Policy Gradients (DDPG) learning algorithm is implemented using pytorch to perform the given task.\n",
    "\n",
    "- The DDPG has 2 networks (local and target) for both actor and critic parts of the agent, making a \n",
    "    total of 4 networks. Each network has 2 fully connected layers (first of size `256` and second of\n",
    "    size `128`) with ReLU activation function, where `input_size` is the number of states in the\n",
    "    environment, here `33`. `output_size` is the number of actions possible for the agent; here `4`.\n",
    "    The activation function for the last layer is tanh for the actor, to ensure output is limited to a\n",
    "    magnitude of 1.\n",
    "    ```\n",
    "        ('fc1', nn.Linear(input_size, fc1_units))\n",
    "        ('reLU', nn.ReLU())\n",
    "        ('fc2', nn.Linear(fc1_units, fc2_units))\n",
    "        ('reLU', nn.ReLU())\n",
    "        ('fc3', nn.Linear(fc2_units, output_size))\n",
    "        ('tanh', nn.tanh())\n",
    "    ```\n",
    "\n",
    "    The Q-network is tasked with the objective to find the optimal policy π* for the agent.\n",
    "    ```\n",
    "    Q(s,a) = Q(s, a) + α[R(s, a) + γmaxQ'(s',a')-Q(s,a)]\n",
    "    ```\n",
    "    \n",
    "- [Ornstein–Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process):\n",
    "    The OU process is used as a noise which helps the agent explore the new possible cases where\n",
    "    the agent can improve itself by learning new samples with (potentially) higher rewards.\n",
    "\n",
    "- Fixed target implementation: A double DQN approach is also adopted where the 2 sets of DQN\n",
    "    are maintained by the agent i.e. local and target. The local network is the one trained at\n",
    "    specific intervals. However, for evaluation of the agent's performance is based on the target\n",
    "    network which slowly converges to local network. This approach ensures that the recently\n",
    "    learned info by the local network doesn't disrupt the overall performance of the agent.\n",
    "    Furthermore, the approach makes the network learn slowly but tends to converge to learn correct\n",
    "    experiences over long periods.\n",
    "\n",
    "- Soft update: To update the weights of the target network, a simple weighted average between\n",
    "    the local and the target network is performed periodically. Depending on the weight, the\n",
    "    target network undergoes a soft update with respect to the high-frequency varying local\n",
    "    network.\n",
    "\n",
    "- Hyperparameters: Several hyperparameters are used in the training/testing process, details\n",
    "    of which are given below. Further, the specific values of the parameters used during\n",
    "    the training session are stored in `log_path` file. Similarly, the graph of score vs episode\n",
    "    is stored as `fig_path` file. Users are requested to examine these two files for the training\n",
    "    performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup parameters (User-input required)\n",
    "\n",
    "Tune the following parameters to obtain necessary results.\n",
    "\n",
    "NOTE: Ensure `train_flag=False` and that `model_path` file is available in the current folder if an already trained model is to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "train_flag = True # Set True if wish to train a network, else set False for testing\n",
    "SEED = 4 # Set user-defined seed to ensure same results, else set False for random seed\n",
    "\n",
    "# Environment parameters\n",
    "qualify_score = 30 # Score at which the training must stop\n",
    "score_window = 100 # Number of episodes for which the qualify score should be maintained as average\n",
    "\n",
    "# QNN parameters\n",
    "ACTOR_FC = [256, 128] # Neurons in the hidden fully connected layers\n",
    "CRITIC_FC = [256, 128] # Neurons in the hidden fully connected layers\n",
    "\n",
    "ACTOR_LR = 1e-4 # Learning rate actor\n",
    "CRITIC_LR = 1e-4 # Learning rate critic\n",
    "\n",
    "WEIGHT_DECAY = 0 # L2 weight decay\n",
    "BATCH_SIZE = 128 # Number of samples/batch for training the Q network\n",
    "\n",
    "# DRL parameters\n",
    "steps_max = 1024 # Maximum number of steps to be taken in an episode\n",
    "train_episodes = 1024 # Number of episodes for which the agent must be trained\n",
    "\n",
    "TAU = 1e-3 # The degree of influence the target network has on the main/local network\n",
    "UPDATE_EVERY = 1 # Number of time-steps in an episode after which the Q network should be updated\n",
    "\n",
    "BUFFER_SIZE = int(1e5) # Number of episodes to keep in memory (experience replay)\n",
    "GAMMA = 0.99 # Discount factor of the rewards\n",
    "\n",
    "# OUNoise parameters\n",
    "THETA = 0.15\n",
    "SIGMA = 0.2\n",
    "\n",
    "# Extra variables for ease of use\n",
    "# app_path = 'Reacher_1_Linux/Reacher.x86_64'\n",
    "app_path = 'Reacher_20_Linux/Reacher.x86_64'\n",
    "\n",
    "log_path = 'output.txt'\n",
    "fig_path = 'output.pdf'\n",
    "\n",
    "model_path = 'checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# NN\n",
    "import torch as tc\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Replay buffer\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# General\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "from time import sleep, time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup the Unity environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20 agents.\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "env = UnityEnvironment(file_name=app_path)\n",
    "device = tc.device(\"cpu\")\n",
    "# device = tc.device(\"cuda:0\" if tc.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# get environment details\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "print('Using {} agents.'.format(num_agents))\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = len(env_info.vector_observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Deep Q-Network\n",
    "\n",
    "The following class builds generic NN models for actor and critic with only FC layers and ReLU activation function (using `pytorch`), except tanh activation for last layer of actor because values need to be clipped in \\[-1,1\\].\n",
    "The size and number of hidden layers are tunable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor_Network(nn.Module):\n",
    "    \"\"\"Actor Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, output_size, seed=False, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"\n",
    "        Build a simple Fully Connected neural network with n hidden layers and ReLU activation.\n",
    "\n",
    "        Param\n",
    "        =====\n",
    "            state_size (int): Dimension of input layer\n",
    "            output_size (int): Dimension of output layer\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Actor_Network, self).__init__()\n",
    "        \n",
    "        if seed:\n",
    "            self.seed = tc.manual_seed(seed)\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return tc.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "\n",
    "class Critic_Network(nn.Module):\n",
    "    \"\"\"Critic Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"\n",
    "        Build a simple Fully Connected neural network with n hidden layers and ReLU activation.\n",
    "\n",
    "        Param\n",
    "        =====\n",
    "            state_size (int): Dimension of state layer\n",
    "            action_size (int): Dimension of action layer\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Critic_Network, self).__init__()\n",
    "        \n",
    "        if seed:\n",
    "            self.seed = tc.manual_seed(seed)\n",
    "        \n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = tc.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ornstein–Uhlenbeck Process Noise\n",
    "OU Process is a Gauss–Markov process and over time, the process tends to drift towards its long-term mean.\n",
    "This is useful for generating Gaussian noise about the state of a dynamic which tends towards the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.size = size        \n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma        \n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state        \n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Replay Buffer\n",
    "\n",
    "Experience replay is a key technique behind many recent advances in deep reinforcement learning.\n",
    "Allowing the agent to learn from earlier memories can speed up learning and break undesirable temporal correlations.\n",
    "A replay buffer stores all the past memories upto `buffer_size` length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed=False):\n",
    "        \"\"\"\n",
    "        Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", \\\n",
    "                                     field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "        if seed:\n",
    "            self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        \n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        if self.batch_size < len(self.memory):\n",
    "            experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        else:\n",
    "            experiences = random.sample(self.memory, k=len(self.memory))\n",
    "\n",
    "        states = tc.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = tc.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = tc.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = tc.from_numpy(np.vstack([e.next_state for e in experiences if e is not None]\\\n",
    "                                             )).float().to(device)\n",
    "        dones = tc.from_numpy(np.vstack([e.done for e in experiences if e is not None]\\\n",
    "                                       ).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Agent\n",
    "\n",
    "Create an agent to play the game and learn from it.\n",
    "Notice that the agent uses both of the above defined classes to create required objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    Interacts with and learns from the environment.\n",
    "    \n",
    "    Global Params used:\n",
    "    ------------------\n",
    "    SEED\n",
    "    ACTOR_FC\n",
    "    CRITIC_FC\n",
    "    ACTOR_LR\n",
    "    CRITIC_LR\n",
    "    BUFFER_SIZE\n",
    "    BATCH_SIZE\n",
    "    UPDATE_EVERY\n",
    "    GAMMA\n",
    "    TAU\n",
    "    \n",
    "    Global Class used:\n",
    "    ------------------\n",
    "    DQN\n",
    "    ReplayBuffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"\n",
    "        Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        if SEED:\n",
    "            self.seed = np.random.seed(SEED)\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor_local = Actor_Network(state_size, action_size, SEED, ACTOR_FC[0], ACTOR_FC[1]).to(device)\n",
    "        self.actor_target = Actor_Network(state_size, action_size, SEED, ACTOR_FC[0], ACTOR_FC[1]).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=ACTOR_LR)\n",
    "        \n",
    "        # Critic Network\n",
    "        self.critic_local = Critic_Network(state_size, action_size, SEED, CRITIC_FC[0], \\\n",
    "                                           CRITIC_FC[1]).to(device)\n",
    "        self.critic_target = Critic_Network(state_size, action_size, SEED, CRITIC_FC[0], \\\n",
    "                                            CRITIC_FC[1]).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=CRITIC_LR, \\\n",
    "                                           weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, SEED)\n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise((num_agents, action_size), SEED)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \n",
    "        # Separate experiences from different \"agents\" into a lot of experiences from same agent\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if (self.t_step == 0) and (len(self.memory) > UPDATE_EVERY):\n",
    "            self.learn()\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"\n",
    "        Returns actions for given state as per current policy.\n",
    "        \"\"\"\n",
    "        state = tc.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with tc.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Update value parameters using given batch of experience tuples.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "        \n",
    "        # 1. Critic local optimize\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # 2. Actor local optimize\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 3. Critic and action target update\n",
    "        self.soft_update(self.critic_local, self.critic_target)\n",
    "        self.soft_update(self.actor_local, self.actor_target)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"\n",
    "        Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(TAU*local_param.data + (1.0-TAU)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Train the network (only if `train_flag=True`)\n",
    "\n",
    "NOTE: Training results are stored as `log_path` and the graph as `fig_path` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Episode: 100 Step: 1000 Score: 37.07\n",
      "Episode: 100 Avg Time: 16.82s Avg Score: 28.02\n",
      "Solved environment in 106 episodes, avg score: 30.19\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an agent which interacts with the environment\n",
    "agent = Agent(state_size, action_size)\n",
    "\n",
    "if train_flag:\n",
    "\n",
    "    # Setup variables\n",
    "    all_scores = []\n",
    "    steps_period = int(steps_max/10)\n",
    "    episode_period = min([int(train_episodes/100), score_window])\n",
    "    start_time = time()\n",
    "\n",
    "    # Log info (such that browser can be safely closed)\n",
    "    text = 'train_flag = {}\\nSEED = {}\\nqualify_score = {}\\nscore_window = {}\\nACTOR_FC = {}\\n' + \\\n",
    "           'CRITIC_FC = {}\\nACTOR_LR = {}\\nCRITIC_LR = {}\\nBATCH_SIZE = {}\\nsteps_max = {}\\n' + \\\n",
    "           'train_episodes = {}\\nTAU = {}\\nUPDATE_EVERY = {}\\nBUFFER_SIZE = {}\\nGAMMA = {}\\n\\n'\n",
    "    text = text.format(train_flag, SEED, qualify_score, score_window, ACTOR_FC, CRITIC_FC, ACTOR_LR, \\\n",
    "                       CRITIC_LR, BATCH_SIZE, steps_max, train_episodes, TAU, UPDATE_EVERY, \\\n",
    "                       BUFFER_SIZE, GAMMA)\n",
    "    with open(log_path,'w+') as file:\n",
    "        file.write(text)\n",
    "\n",
    "    for e in range(train_episodes):\n",
    "\n",
    "        # Reset environment and setup variables\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        states = env_info.vector_observations # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents) # initialize the score (for each agent)\n",
    "        solved_flag = False\n",
    "\n",
    "        for t in range(steps_max):\n",
    "            # Get next action from the agent\n",
    "            actions = agent.act(states)\n",
    "            actions = np.clip(actions, -1, 1)              # all actions between -1 and 1\n",
    "\n",
    "            # Run one step and collect env info\n",
    "            env_info = env.step(actions)[brain_name]       # send the action to the environment\n",
    "            next_states = env_info.vector_observations     # get the next state\n",
    "            rewards = env_info.rewards                     # get the reward\n",
    "            dones = env_info.local_done                    # see if episode has finished\n",
    "            scores += rewards\n",
    "\n",
    "            # Check if required average score is obtained\n",
    "            if t % steps_period == 0:\n",
    "                text = \"\\t Episode: {} Step: {:03} Score: {:0.2f}  \".format(e, t, np.mean(scores))\n",
    "                print(text, end = '\\r')\n",
    "                with open(log_path,'a+') as file:\n",
    "                    file.write(text+'\\n')\n",
    "\n",
    "            if any(dones):\n",
    "                    break\n",
    "\n",
    "            # Prepare for the next step\n",
    "            agent.step(states, actions, rewards, next_states, dones) # Store the information of last step\n",
    "            states = next_states # Update states\n",
    "\n",
    "        # Check if required average score is obtained\n",
    "        text = \"\\t Episode: {} Step: {:03} Score: {:0.2f}  \".format(e, t, np.mean(scores))\n",
    "        print(text, end = '\\r')\n",
    "        with open(log_path,'a+') as file:\n",
    "            file.write(text+'\\n')\n",
    "\n",
    "        # Check if required average score is obtained\n",
    "        all_scores.append(np.mean(scores))\n",
    "        if len(all_scores) > score_window:\n",
    "            if e % episode_period == 0:\n",
    "                text = \"Episode: {} Avg Time: {:0.2f}s Avg Score: {:0.2f}\".format(e, (time()-start_time)/e, \\\n",
    "                                                                            np.mean(all_scores[-score_window:]))\n",
    "                with open(log_path,'a+') as file:\n",
    "                    file.write(text+'\\n')\n",
    "\n",
    "                print()\n",
    "                print(text)\n",
    "                \n",
    "                # Plot the scores - for better viz, than just texts\n",
    "                fig = plt.figure()\n",
    "                plt.plot(np.arange(len(all_scores)), all_scores)\n",
    "                plt.ylabel('Score')\n",
    "                plt.xlabel('Episode #')\n",
    "                plt.grid()\n",
    "                fig.savefig(fig_path, bbox_inches='tight') # Vector graphics for detailed viz\n",
    "                plt.close(fig)\n",
    "                \n",
    "            if np.mean(all_scores[-score_window:]) > qualify_score:\n",
    "                text = \"Solved environment in {} episodes, avg score: {:0.2f}\".format(\\\n",
    "                    e, np.mean(all_scores[-score_window:]))\n",
    "\n",
    "                with open(log_path,'a+') as file:\n",
    "                    file.write(text+'\\n')\n",
    "                \n",
    "                print(text)\n",
    "                \n",
    "                tc.save(agent.actor_local.state_dict(), str(num_agents) + '_actor_' + model_path)\n",
    "                tc.save(agent.critic_local.state_dict(), str(num_agents) + '_critic_' + model_path)\n",
    "                \n",
    "                solved_flag = True\n",
    "                \n",
    "                break\n",
    "        \n",
    "    # Plot the scores\n",
    "    fig = plt.figure()\n",
    "    plt.plot(np.arange(len(all_scores)), all_scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    if solved_flag:\n",
    "        plt.title(text)\n",
    "    else:\n",
    "        plt.title(text[2:])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    fig.savefig(fig_path, bbox_inches='tight') # Vector graphics for detailed viz\n",
    "    fig.savefig('rewards.png', bbox_inches='tight') # Raster graphics to show later in Section 9\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Test the performance (only if `train_flag=False`)\n",
    "\n",
    "Test an already trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_flag:\n",
    "    \n",
    "    # Initialization\n",
    "    agent.actor_local.load_state_dict(tc.load(str(num_agents)+'_actor_'+model_path)) # load the weights from file\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    states = env_info.vector_observations # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents) # initialize the score (for each agent)\n",
    "\n",
    "    while True:\n",
    "        actions = agent.act(states) # select an action (for each agent)\n",
    "        env_info = env.step(actions)[brain_name] # send the action to the environment\n",
    "        scores += env_info.rewards # get the reward (for each agent)\n",
    "        dones = env_info.local_done # see if episode has finished (for each agent)\n",
    "        if np.any(dones): # exit loop if episode finished (for any agent)\n",
    "            break\n",
    "        next_states = env_info.vector_observations # get the next state (for each agent)\n",
    "        states = next_states # roll over the state to next time step (for each agent)\n",
    "\n",
    "    print('Total score (averaged over agents) in this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "### Plot of training rewards\n",
    "\n",
    "![reacher_agent_rewards](rewards.png)\n",
    "\n",
    "NOTE: The above figure might not be the true figure due to disk cache problem. Please see `fig_path` file for the latest and true graph.\n",
    "\n",
    "### Observations\n",
    "\n",
    "There were a lot of observations and learnings during the training sessions:\n",
    "- Unfortunately, my biggest observation is luck. Seriously, getting the right combination of parameters\n",
    "    can be very tricky. Fortunately, there exists a lot of parameter tuning libraries to help with this\n",
    "    problem. Nonetheless, this is a bitter-sweet part of a DL and especially, DRL algorithms.\n",
    "- Sparse rewards are very bad for agent's performance. This is because agent learns the `0` rewards from\n",
    "    experiences more often than the sparse rewards. OUNoise really helps increase the chances of observing\n",
    "    good experiences.\n",
    "- Some of the parameters affect the system tremendously. For example, changing the SEED might just be enough\n",
    "    to get the job done. Typically though, a special combination of parameters is required for each environment\n",
    "    in order to train the system well.\n",
    "- I've personally tried almost everything I could think of, including LSTM, priority sampling, manual tuning of\n",
    "    network size, learning rates, batch normalization, same weights for actor and critic, manual/dynamic tuning\n",
    "    of `sigma` in OUNoise, negative rewards for extreme actions, etc. What actually worked in the end is to\n",
    "    keep the network small (only 2 layers), weight decay = 0 (such that the network doesn't forget sparse\n",
    "    rewards) and trying out different seed values. (Sounds too simple, but sometimes, that's all\n",
    "    required in life ;-)\n",
    "\n",
    "### Future Ideas\n",
    "\n",
    "Especially for continuous action-space, a lot of novel algorithms are being developed such as SAC, A3C, A2C, PPO, Rainbow, etc. Therefore, it would be worth trying such algorithms, or a mixture of them to potentially improve the agent's performance. Furthermore, thanks to Unity and other developers, we should explore several environments (apart from double joint limb) as testbeds for the aforementioned algorithms.\n",
    "\n",
    "In practice, more than the DRL algorithm, many other factors play significant role in agent's performance such as the state space reqresentation, reward function, good sample experiences from an expert, evolution theories to tune the parameters, etc. Thus, a DRL agent as such must be designed with the whole complete system in mind and not just the DRL algorithm. Such explorations will be examined in future."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
